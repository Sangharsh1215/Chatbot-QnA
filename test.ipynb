{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_qa.txt\", \"rb\") as fp:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_qa.txt\", \"rb\") as fp:  \n",
    "    test_data =  pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = test_data + train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for story, question , answer in all_data:\n",
    "    vocab = vocab.union(set(story))\n",
    "    vocab = vocab.union(set(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.add('no')\n",
    "vocab.add('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " '?',\n",
       " 'Daniel',\n",
       " 'Is',\n",
       " 'John',\n",
       " 'Mary',\n",
       " 'Sandra',\n",
       " 'apple',\n",
       " 'back',\n",
       " 'bathroom',\n",
       " 'bedroom',\n",
       " 'discarded',\n",
       " 'down',\n",
       " 'dropped',\n",
       " 'football',\n",
       " 'garden',\n",
       " 'got',\n",
       " 'grabbed',\n",
       " 'hallway',\n",
       " 'in',\n",
       " 'journeyed',\n",
       " 'kitchen',\n",
       " 'left',\n",
       " 'milk',\n",
       " 'moved',\n",
       " 'no',\n",
       " 'office',\n",
       " 'picked',\n",
       " 'put',\n",
       " 'the',\n",
       " 'there',\n",
       " 'to',\n",
       " 'took',\n",
       " 'travelled',\n",
       " 'up',\n",
       " 'went',\n",
       " 'yes'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(vocab) + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_story_len = max([len(data[0]) for data in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_story_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_question_len = max([len(data[1]) for data in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters=[])\n",
    "tokenizer.fit_on_texts(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grabbed': 1,\n",
       " 'office': 2,\n",
       " 'put': 3,\n",
       " 'bathroom': 4,\n",
       " 'yes': 5,\n",
       " 'took': 6,\n",
       " 'journeyed': 7,\n",
       " 'back': 8,\n",
       " 'kitchen': 9,\n",
       " 'there': 10,\n",
       " '?': 11,\n",
       " 'dropped': 12,\n",
       " 'milk': 13,\n",
       " 'is': 14,\n",
       " 'john': 15,\n",
       " 'in': 16,\n",
       " 'down': 17,\n",
       " 'went': 18,\n",
       " 'mary': 19,\n",
       " 'hallway': 20,\n",
       " 'left': 21,\n",
       " 'sandra': 22,\n",
       " 'the': 23,\n",
       " 'moved': 24,\n",
       " 'no': 25,\n",
       " 'apple': 26,\n",
       " 'to': 27,\n",
       " 'discarded': 28,\n",
       " 'up': 29,\n",
       " 'got': 30,\n",
       " 'picked': 31,\n",
       " 'travelled': 32,\n",
       " 'daniel': 33,\n",
       " 'garden': 34,\n",
       " 'bedroom': 35,\n",
       " '.': 36,\n",
       " 'football': 37}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_story_text = []\n",
    "train_question_text = []\n",
    "train_answers = []\n",
    "\n",
    "for story,question,answer in train_data:\n",
    "    train_story_text.append(story)\n",
    "    train_question_text.append(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_story_seq = tokenizer.texts_to_sequences(train_story_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_story_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_stories(data, word_index=tokenizer.word_index, max_story_len=max_story_len,max_question_len=max_question_len):\n",
    "    X = []\n",
    "    Xq = []\n",
    "    Y = []\n",
    "    for story, query, answer in data:\n",
    "        x = [word_index[word.lower()] for word in story]\n",
    "        xq = [word_index[word.lower()] for word in query]\n",
    "        y = np.zeros(len(word_index) + 1)\n",
    "        y[word_index[answer]] = 1\n",
    "        \n",
    "       \n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "  \n",
    "    return (pad_sequences(X, maxlen=max_story_len),pad_sequences(Xq, maxlen=max_question_len), np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train, queries_train, answers_train = vectorize_stories(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test, queries_test, answers_test = vectorize_stories(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ..., 23, 35, 36],\n",
       "       [ 0,  0,  0, ..., 23, 34, 36],\n",
       "       [ 0,  0,  0, ..., 23, 34, 36],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ..., 23, 26, 36],\n",
       "       [ 0,  0,  0, ..., 23, 34, 36],\n",
       "       [ 0,  0,  0, ..., 26, 10, 36]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14, 15, 16, 23,  9, 11],\n",
       "       [14, 15, 16, 23,  9, 11],\n",
       "       [14, 15, 16, 23, 34, 11],\n",
       "       ...,\n",
       "       [14, 19, 16, 23, 35, 11],\n",
       "       [14, 22, 16, 23, 34, 11],\n",
       "       [14, 19, 16, 23, 34, 11]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0., 497.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0., 503.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(answers_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
    "from keras.layers import add, dot, concatenate\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = Input((max_story_len,))\n",
    "question = Input((max_question_len,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_size,output_dim=64))\n",
    "input_encoder_m.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_size,output_dim=max_question_len))\n",
    "input_encoder_c.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                               output_dim=64,\n",
    "                               input_length=max_question_len))\n",
    "question_encoder.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "question_encoded = question_encoder(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
    "match = Activation('softmax')(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n",
    "response = Permute((2, 1))(response)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = concatenate([response, question_encoded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 6, 220) dtype=float32 (created by layer 'concatenate_1')>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = LSTM(32)(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = Dropout(0.5)(answer)\n",
    "answer = Dense(vocab_size)(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = Activation('softmax')(answer)\n",
    "\n",
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 156)]                0         []                            \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)        [(None, 6)]                  0         []                            \n",
      "                                                                                                  \n",
      " sequential_3 (Sequential)   (None, None, 64)             2432      ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " sequential_5 (Sequential)   (None, 6, 64)                2432      ['input_4[0][0]']             \n",
      "                                                                                                  \n",
      " dot_1 (Dot)                 (None, 156, 6)               0         ['sequential_3[0][0]',        \n",
      "                                                                     'sequential_5[0][0]']        \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 156, 6)               0         ['dot_1[0][0]']               \n",
      "                                                                                                  \n",
      " sequential_4 (Sequential)   (None, None, 6)              228       ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 156, 6)               0         ['activation_1[0][0]',        \n",
      "                                                                     'sequential_4[0][0]']        \n",
      "                                                                                                  \n",
      " permute_1 (Permute)         (None, 6, 156)               0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 6, 220)               0         ['permute_1[0][0]',           \n",
      " )                                                                   'sequential_5[0][0]']        \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 (None, 32)                   32384     ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 32)                   0         ['lstm[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 38)                   1254      ['dropout_6[0][0]']           \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 38)                   0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 38730 (151.29 KB)\n",
      "Trainable params: 38730 (151.29 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "313/313 [==============================] - 5s 9ms/step - loss: 0.9159 - accuracy: 0.4987 - val_loss: 0.6957 - val_accuracy: 0.4970\n",
      "Epoch 2/100\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.7081 - accuracy: 0.4992 - val_loss: 0.6934 - val_accuracy: 0.4990\n",
      "Epoch 3/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.6988 - accuracy: 0.4989 - val_loss: 0.6947 - val_accuracy: 0.4970\n",
      "Epoch 4/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6956 - accuracy: 0.4966 - val_loss: 0.6932 - val_accuracy: 0.5030\n",
      "Epoch 5/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6959 - accuracy: 0.5008 - val_loss: 0.6934 - val_accuracy: 0.4970\n",
      "Epoch 6/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6951 - accuracy: 0.5005 - val_loss: 0.6932 - val_accuracy: 0.5030\n",
      "Epoch 7/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6952 - accuracy: 0.4997 - val_loss: 0.6942 - val_accuracy: 0.5030\n",
      "Epoch 8/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6950 - accuracy: 0.4999 - val_loss: 0.6936 - val_accuracy: 0.5030\n",
      "Epoch 9/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6956 - accuracy: 0.4984 - val_loss: 0.6967 - val_accuracy: 0.4970\n",
      "Epoch 10/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6951 - accuracy: 0.5009 - val_loss: 0.6936 - val_accuracy: 0.5030\n",
      "Epoch 11/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6951 - accuracy: 0.5046 - val_loss: 0.6944 - val_accuracy: 0.5030\n",
      "Epoch 12/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6948 - accuracy: 0.5026 - val_loss: 0.6932 - val_accuracy: 0.5030\n",
      "Epoch 13/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6955 - accuracy: 0.5019 - val_loss: 0.6938 - val_accuracy: 0.4970\n",
      "Epoch 14/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6956 - accuracy: 0.4971 - val_loss: 0.6947 - val_accuracy: 0.4970\n",
      "Epoch 15/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6949 - accuracy: 0.5027 - val_loss: 0.6949 - val_accuracy: 0.4970\n",
      "Epoch 16/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6949 - accuracy: 0.4964 - val_loss: 0.6931 - val_accuracy: 0.5030\n",
      "Epoch 17/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6947 - accuracy: 0.4992 - val_loss: 0.6937 - val_accuracy: 0.5030\n",
      "Epoch 18/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6953 - accuracy: 0.5028 - val_loss: 0.6935 - val_accuracy: 0.5030\n",
      "Epoch 19/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6955 - accuracy: 0.4972 - val_loss: 0.6931 - val_accuracy: 0.5030\n",
      "Epoch 20/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6949 - accuracy: 0.5010 - val_loss: 0.6940 - val_accuracy: 0.5030\n",
      "Epoch 21/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6954 - accuracy: 0.4980 - val_loss: 0.6932 - val_accuracy: 0.5030\n",
      "Epoch 22/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6952 - accuracy: 0.4967 - val_loss: 0.6941 - val_accuracy: 0.5030\n",
      "Epoch 23/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6952 - accuracy: 0.4969 - val_loss: 0.6931 - val_accuracy: 0.5030\n",
      "Epoch 24/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6947 - accuracy: 0.5025 - val_loss: 0.6932 - val_accuracy: 0.5040\n",
      "Epoch 25/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6951 - accuracy: 0.4965 - val_loss: 0.6941 - val_accuracy: 0.4970\n",
      "Epoch 26/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6956 - accuracy: 0.4980 - val_loss: 0.6940 - val_accuracy: 0.4970\n",
      "Epoch 27/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6949 - accuracy: 0.4926 - val_loss: 0.6932 - val_accuracy: 0.4970\n",
      "Epoch 28/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6954 - accuracy: 0.4995 - val_loss: 0.6956 - val_accuracy: 0.4970\n",
      "Epoch 29/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6950 - accuracy: 0.4953 - val_loss: 0.6937 - val_accuracy: 0.4970\n",
      "Epoch 30/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6943 - accuracy: 0.5061 - val_loss: 0.6932 - val_accuracy: 0.5030\n",
      "Epoch 31/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6946 - accuracy: 0.5016 - val_loss: 0.6942 - val_accuracy: 0.4970\n",
      "Epoch 32/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6954 - accuracy: 0.4973 - val_loss: 0.6941 - val_accuracy: 0.4970\n",
      "Epoch 33/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6945 - accuracy: 0.5042 - val_loss: 0.6932 - val_accuracy: 0.5030\n",
      "Epoch 34/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6952 - accuracy: 0.5023 - val_loss: 0.6938 - val_accuracy: 0.4970\n",
      "Epoch 35/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6950 - accuracy: 0.5031 - val_loss: 0.6932 - val_accuracy: 0.5030\n",
      "Epoch 36/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6950 - accuracy: 0.5010 - val_loss: 0.6966 - val_accuracy: 0.5030\n",
      "Epoch 37/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6947 - accuracy: 0.5070 - val_loss: 0.6935 - val_accuracy: 0.5030\n",
      "Epoch 38/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6951 - accuracy: 0.5032 - val_loss: 0.6931 - val_accuracy: 0.5030\n",
      "Epoch 39/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6945 - accuracy: 0.5021 - val_loss: 0.6940 - val_accuracy: 0.4970\n",
      "Epoch 40/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6950 - accuracy: 0.5025 - val_loss: 0.6943 - val_accuracy: 0.5030\n",
      "Epoch 41/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.6946 - accuracy: 0.5027 - val_loss: 0.6935 - val_accuracy: 0.5030\n",
      "Epoch 42/100\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.6948 - accuracy: 0.4999 - val_loss: 0.6932 - val_accuracy: 0.4970\n",
      "Epoch 43/100\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.6947 - accuracy: 0.4997 - val_loss: 0.6933 - val_accuracy: 0.4970\n",
      "Epoch 44/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.6948 - accuracy: 0.4981 - val_loss: 0.6948 - val_accuracy: 0.5030\n",
      "Epoch 45/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.6950 - accuracy: 0.4944 - val_loss: 0.6937 - val_accuracy: 0.4970\n",
      "Epoch 46/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.6947 - accuracy: 0.5003 - val_loss: 0.6933 - val_accuracy: 0.5030\n",
      "Epoch 47/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.6949 - accuracy: 0.4973 - val_loss: 0.6933 - val_accuracy: 0.4970\n",
      "Epoch 48/100\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.6944 - accuracy: 0.5021 - val_loss: 0.6931 - val_accuracy: 0.5030\n",
      "Epoch 49/100\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.6943 - accuracy: 0.5100 - val_loss: 0.6945 - val_accuracy: 0.5030\n",
      "Epoch 50/100\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.6946 - accuracy: 0.5096 - val_loss: 0.6932 - val_accuracy: 0.4970\n",
      "Epoch 51/100\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.6950 - accuracy: 0.5005 - val_loss: 0.6940 - val_accuracy: 0.5030\n",
      "Epoch 52/100\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6948 - accuracy: 0.5052 - val_loss: 0.6933 - val_accuracy: 0.5030\n",
      "Epoch 53/100\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.6947 - accuracy: 0.5004 - val_loss: 0.6960 - val_accuracy: 0.4970\n",
      "Epoch 54/100\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.6944 - accuracy: 0.5013 - val_loss: 0.6933 - val_accuracy: 0.5030\n",
      "Epoch 55/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.6946 - accuracy: 0.5073 - val_loss: 0.6932 - val_accuracy: 0.4970\n",
      "Epoch 56/100\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.6942 - accuracy: 0.5071 - val_loss: 0.6938 - val_accuracy: 0.4970\n",
      "Epoch 57/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6944 - accuracy: 0.4993 - val_loss: 0.6932 - val_accuracy: 0.5030\n",
      "Epoch 58/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.6954 - accuracy: 0.4935 - val_loss: 0.6932 - val_accuracy: 0.4970\n",
      "Epoch 59/100\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.6950 - accuracy: 0.4941 - val_loss: 0.6933 - val_accuracy: 0.5030\n",
      "Epoch 60/100\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.6950 - accuracy: 0.4928 - val_loss: 0.6932 - val_accuracy: 0.4970\n",
      "Epoch 61/100\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.6947 - accuracy: 0.5038 - val_loss: 0.6933 - val_accuracy: 0.4970\n",
      "Epoch 62/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.6946 - accuracy: 0.5026 - val_loss: 0.6934 - val_accuracy: 0.4970\n",
      "Epoch 63/100\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.6955 - accuracy: 0.4896 - val_loss: 0.6939 - val_accuracy: 0.5030\n",
      "Epoch 64/100\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.6945 - accuracy: 0.5019 - val_loss: 0.6944 - val_accuracy: 0.4970\n",
      "Epoch 65/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.6949 - accuracy: 0.4972 - val_loss: 0.6934 - val_accuracy: 0.4970\n",
      "Epoch 66/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6947 - accuracy: 0.5051 - val_loss: 0.6935 - val_accuracy: 0.5030\n",
      "Epoch 67/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6943 - accuracy: 0.4997 - val_loss: 0.6932 - val_accuracy: 0.4970\n",
      "Epoch 68/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6950 - accuracy: 0.4953 - val_loss: 0.6932 - val_accuracy: 0.4970\n",
      "Epoch 69/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.6939 - accuracy: 0.5015 - val_loss: 0.6982 - val_accuracy: 0.4970\n",
      "Epoch 70/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6953 - accuracy: 0.4952 - val_loss: 0.6951 - val_accuracy: 0.5030\n",
      "Epoch 71/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.6951 - accuracy: 0.4930 - val_loss: 0.6933 - val_accuracy: 0.5030\n",
      "Epoch 72/100\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.6947 - accuracy: 0.4988 - val_loss: 0.6932 - val_accuracy: 0.4970\n",
      "Epoch 73/100\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.6943 - accuracy: 0.5027 - val_loss: 0.6939 - val_accuracy: 0.4970\n",
      "Epoch 74/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.6941 - accuracy: 0.5108 - val_loss: 0.6951 - val_accuracy: 0.4970\n",
      "Epoch 75/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.6943 - accuracy: 0.5045 - val_loss: 0.6932 - val_accuracy: 0.5030\n",
      "Epoch 76/100\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.6948 - accuracy: 0.5009 - val_loss: 0.6932 - val_accuracy: 0.5080\n",
      "Epoch 77/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6947 - accuracy: 0.5000 - val_loss: 0.6951 - val_accuracy: 0.5030\n",
      "Epoch 78/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.6945 - accuracy: 0.5014 - val_loss: 0.6930 - val_accuracy: 0.5120\n",
      "Epoch 79/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6941 - accuracy: 0.5048 - val_loss: 0.6963 - val_accuracy: 0.4970\n",
      "Epoch 80/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6945 - accuracy: 0.4994 - val_loss: 0.6930 - val_accuracy: 0.5190\n",
      "Epoch 81/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6937 - accuracy: 0.5078 - val_loss: 0.6931 - val_accuracy: 0.5170\n",
      "Epoch 82/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6937 - accuracy: 0.5152 - val_loss: 0.6929 - val_accuracy: 0.5130\n",
      "Epoch 83/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6940 - accuracy: 0.5087 - val_loss: 0.6931 - val_accuracy: 0.5060\n",
      "Epoch 84/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6928 - accuracy: 0.5207 - val_loss: 0.6927 - val_accuracy: 0.5090\n",
      "Epoch 85/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.6921 - accuracy: 0.5185 - val_loss: 0.6926 - val_accuracy: 0.5150\n",
      "Epoch 86/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6931 - accuracy: 0.5123 - val_loss: 0.6927 - val_accuracy: 0.5190\n",
      "Epoch 87/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.6926 - accuracy: 0.5196 - val_loss: 0.6929 - val_accuracy: 0.5270\n",
      "Epoch 88/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6910 - accuracy: 0.5266 - val_loss: 0.6924 - val_accuracy: 0.5030\n",
      "Epoch 89/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.6914 - accuracy: 0.5226 - val_loss: 0.6900 - val_accuracy: 0.5260\n",
      "Epoch 90/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6868 - accuracy: 0.5397 - val_loss: 0.6855 - val_accuracy: 0.5580\n",
      "Epoch 91/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6806 - accuracy: 0.5509 - val_loss: 0.6643 - val_accuracy: 0.5890\n",
      "Epoch 92/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6667 - accuracy: 0.5707 - val_loss: 0.6512 - val_accuracy: 0.6070\n",
      "Epoch 93/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.6580 - accuracy: 0.5894 - val_loss: 0.6330 - val_accuracy: 0.6340\n",
      "Epoch 94/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.6429 - accuracy: 0.6206 - val_loss: 0.6216 - val_accuracy: 0.6410\n",
      "Epoch 95/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6241 - accuracy: 0.6525 - val_loss: 0.5884 - val_accuracy: 0.6860\n",
      "Epoch 96/100\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.6035 - accuracy: 0.6836 - val_loss: 0.5689 - val_accuracy: 0.7040\n",
      "Epoch 97/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.5843 - accuracy: 0.7000 - val_loss: 0.5462 - val_accuracy: 0.7060\n",
      "Epoch 98/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.5579 - accuracy: 0.7253 - val_loss: 0.5125 - val_accuracy: 0.7580\n",
      "Epoch 99/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.5333 - accuracy: 0.7418 - val_loss: 0.5123 - val_accuracy: 0.7530\n",
      "Epoch 100/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.5142 - accuracy: 0.7615 - val_loss: 0.4793 - val_accuracy: 0.7880\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([inputs_train, queries_train], answers_train,batch_size=32,epochs=100,validation_data=([inputs_test, queries_test], answers_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
